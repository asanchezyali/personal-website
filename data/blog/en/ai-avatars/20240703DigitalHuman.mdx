---
title: 'Revolutionize Animation: Build a Digital Human with Large Language Models'
date: '2024/07/03'
lastmod: '2024-07-03'
tags: ['ai-avatar', 'digital-human', 'artificial intelligence']
draft: false
time: '47 min'
summary: 'A Step-by-Step Guide to Creating Your Next AI-Powered Avatar.'
layout: PostLayout
bibliography: references-data.bib
canonicalUrl: https://www.asanchezyali.com/blog/en/ml/20210712MachineLearning/
headerImage: '/images/ai-avatars/header-post2.png'
language: 'en'
---

_Written by Alejandro Sánchez Yalí with Jenalee Prentice. Originally published 2024-07-03
on the [Monadical blog](https://monadical.com/blog.html)._

## Introduction

There's a new generation of avatars on the block, commonly known as digital humans or AI avatars[^1]. These avatars have
earned this somewhat serious name because they simulate human appearance and behavior in a remarkably realistic manner.
For instance, they're often designed with the imperfect traits, like an uneven skin tone or asymmetrical face. They also
mimic human’s natural language skills by simulating movements and nonverbal cues through sentiment analysis generated by
Large Language Models (LLMs)[^2].

During 2023, the convergence of 2D and 3D modelling technologies with AI tools like Generative Adversarial Networks
(GANs), Text-To-Speech (TTS), Speech-To-Text (STT), and LLMs brought these avatars into our lives, potentially
redefining our interaction with computers.

Take, for instance, [Victoria Shi](https://cybernews.com/news/ukraine-appoints-ai-avatar-spokesperson/), the digital
Ukrainian minister of foreign affairs, who saves the Ukraine time and resources by providing essential updates on
consular affairs. Or, the AI avatars used by companies to interact with customers across various industries such as
[financial consultation](https://www.deepbrain.io/solutions/finance), [educational support](https://praktika.ai/),
[media engagement](https://www.digitalhumans.com/case-studies/digital-einstein), [video games](https://inworld.ai/), and
[health and wellness](https://www.digitalhumans.com/case-studies/groov).

## Tutorial Overview

To better understand the potential of AI avatars, this tutorial will teach you how to build one from A to Z. Whether you're a passionate gamer, an entrepreneur seeking innovation, or a future AI-avatar investor, this tutorial will provide you with all the tools you need to create and understand how these fascinating digital avatars work.

In my previous tutorial, [Kraken the Code: How to Build a Talking Avatar](https://monadical.com/posts/how-to-build-a-talking-avatar-with-azure-cognitive-langchain-and-openai.html), I explained how to implement a 2D-talking avatar. This time, we'll raise the stakes and learn how to implement a 3D-talking AI avatar with enhanced features.

The final code of this tutorial can be found in this repository: [Digital
Human](https://github.com/asanchezyali/talking-avatar-with-ai/tree/article-1.0).

## Key Elements of Our AI Avatar

By the end of this tutorial, we'll have created an AI avatar that stands out amongst its counterparts. This will be achieved by using [GLTF models](https://es.wikipedia.org/wiki/GlTF), [Mixamo](https://www.mixamo.com/) and LLMs to animate and enhance our avatar's lip synchronization and communication abilities. In other words, we aim to build an avatar that looks and sounds like a human when conversing. Let's look at the specific features that we'll incorporate into our avatar to reach this objective:

- **Hyper-realistic appearance:** AI avatars are becoming increasingly realistic. Although they are often created in a studio and then synthesized using machine learning programs, it's becoming easier and faster to create digital avatars using only [GLTF models](https://es.wikipedia.org/wiki/GlTF), photos, or videos recorded by oneself ([see Meshcapade editor](https://me.meshcapade.com/editor)). For our tutorial, we will use an avatar in GLTF format generated by [Ready Player Me](https://readyplayer.me/) rendered through React.js with [React Three Fiber](https://docs.pmnd.rs/react-three-fiber/getting-started/introduction).

- **Natural body movements:** To generate some body movements of the avatar, we will use [Mixamo](https://www.mixamo.com/). As for synchronizing voice with smooth and natural mouth movements, we will use [Rhubarb lip-sync](https://github.com/DanielSWolf/rhubarb-lip-sync/releases). It is a CLI command that generates [phonemes](https://en.wikipedia.org/wiki/Phoneme) locally, and then we use it to synchronize mouth movements with voice.

- **Text-to-speech (TTS):** Text-to-speech (TTS) technology has come a long way since the early days of voice assistants like the [Microsoft Agent](https://en.wikipedia.org/wiki/Microsoft_Agent) and [DECtalk](https://en.wikipedia.org/wiki/DECtalk) in the 1990s. Nowadays, TTS can produce realistic voices in all languages. It can generate a digital human with convincing, natural speech if combined with lip synchronization. In this case, we will use [ElevenLabs](https://elevenlabs.io/), a multi-language text-to-speech service.

- **Speech-to-text (STT):** Speech recognition is a critical feature that allows users to interact with digital avatars through voice commands. For this, we will use [Whisper](https://platform.openai.com/docs/guides/speech-to-text), an OpenAI Speech-to-Text (STT) API. This system transforms the user's voice into text so AI can hear it appropriately.

- **Information processing and understanding:** By leveraging large language models (LLMs) such as GPT-4, we can enhance the AI avatar’s ability to process and understand information. LLMs are trained on vast amounts of data and can provide contextually relevant responses, enhancing interaction between users and digital avatars.

- **Response latency:** The interaction between humans and AI avatars is still not as smooth as a real human interaction. In this case, our avatar will still have high latency– a feature we'll improve upon in a future post.

## Project Setup

The architecture of our system has two directories: the `frontend` directory, which is in charge of rendering the avatar, and the `backend` directory, which is responsible for coordinating all the artificial intelligence services.

```shell
digital-human
├── backend
└── frontend
```

We start by developing a React project from scratch inside the `frontend` directory, using Tailwind CSS and Vite as support. We assume that the reader has previous experience with React. However, if you need guidance on how to start a project with React, Tailwind CSS, and Vite, we suggest you consult the following post: [How to Setup Tailwind CSS in React JS with VS Code](https://dev.to/david_bilsonn/how-to-setup-tailwind-css-in-react-js-with-vs-code-59p4).

After installing the React project, update the structure to resemble the following:

```shell
├── index.html
├── package.json
├── postcss.config.js
├── public
│   ├── animations
│   │   ├── angry.fbx
│   │   ├── defeated.fbx
│   │   ├── dismissing_gesture.fbx
│   │   ├── happy_idle.fbx
│   │   ├── idle.fbx
│   │   ├── sad_idle.fbx
│   │   ├── surprised.fbx
│   │   ├── talking.fbx
│   │   ├── talking_one.fbx
│   │   ├── talking_two.fbx
│   │   └── thoughtful_head_shake.fbx
│   ├── favicon.ico
│   ├── models
│   │   ├── animations.glb
│   │   ├── animations.gltf
│   │   ├── animations_data.bin
│   │   ├── avatar.fbm
│   │   ├── avatar.fbx
│   │   └── avatar.glb
│   └── vite.svg
├── src
│   ├── App.jsx
│   ├── components
│   │   ├── Avatar.jsx
│   │   ├── ChatInterface.jsx
│   │   └── Scenario.jsx
│   ├── constants
│   │   ├── facialExpressions.js
│   │   ├── morphTargets.js
│   │   └── visemesMapping.js
│   ├── hooks
│   │   └── useSpeech.jsx
│   ├── index.css
│   └── main.jsx
├── tailwind.config.js
├── vite.config.js
└── yarn.lock
```

And we install the following dependencies:

```shell
$ yarn add three
$ yarn add @types/three
$ yarn add @react-three/fiber
$ yarn add @react-three/drei
$ yarn add --dev leva
```

As for the backend, we're going to structure it as follows:

```shell
├── audios/
├── bin
│   ├── CHANGELOG.md
│   ├── LICENSE.md
│   ├── README.adoc
│   ├── extras
│   │   ├── AdobeAfterEffects
│   │   ├── EsotericSoftwareSpine
│   │   └── MagixVegas
│   ├── include
│   │   ├── gmock
│   │   └── gtest
│   ├── lib
│   │   ├── cmake
│   │   ├── libgmock.a
│   │   ├── libgmock_main.a
│   │   ├── libgtest.a
│   │   ├── libgtest_main.a
│   │   └── pkgconfig
│   ├── res
│   │   └── sphinx
│   ├── rhubarb
│   └── tests
│       └── resources
├── index.js
├── modules
│   ├── defaultMessages.mjs
│   ├── elevenLabs.mjs
│   ├── lip-sync.mjs
│   ├── openAI.mjs
│   ├── rhubarbLipSync.mjs
│   └── whisper.mjs
├── package.json
├── tmp
├── utils
│   ├── audios.mjs
│   └── files.mjs
└── yarn.lock
```

And we install the following dependencies:

```shell
yarn add elevenlabs-node
yarn add langchain
yarn add @langchain/openai
yarn add zod
```

In the `frontend/src/App.js`, we will implement the following base code:

```javascript showLineNumbers
// frontend/src/App.jsx

import { Loader } from '@react-three/drei'
import { Canvas } from '@react-three/fiber'
import { Leva } from 'leva'
import { Scenario } from './components/Scenario'
import { ChatInterface } from './components/ChatInterface'

function App() {
  return (
    <>
      <Loader />
      <Leva collapsed />
      <ChatInterface />
      <Canvas shadows camera={{ position: [0, 0, 0], fov: 10 }}>
        <Scenario />
      </Canvas>
    </>
  )
}

export default App
```

In the above code, we have set up the main components of a React application integrating 3D graphics using [Three.js](https://threejs.org/) along with the `@react-three/fiber` library. We started by importing Loader from `@react-three/drei` to handle the visualization of loading 3D resources, ensuring a smooth user experience during the application startup.

To represent the 3D scene, we used the Canvas component from `@react-three/fiber`, which serves as a container for all the 3D visual elements. In this case, the Canvas has shadows enabled (`shadows`), and the camera is configured with an initial position of `[0, 0, 0]` and a field of view `(fov)` of 10. The camera has been set to be at the scene's center, providing a neutral initial view for the user.

Additionally, we have integrated [`LEVA`](https://github.com/pmndrs/leva), a tool that provides a control panel for
manipulating variables in real time without changing the code.

<ImageBox src="/images/ai-avatars/fig1v2.png" alt="LEVA control panel" width="728px" height="368px">
  **Figura 1**. LEVA control panel.
</ImageBox>

The `Scenario` component represents the core of our 3D scene, where all objects, lights, and specific environment configurations are placed. This component looks like:

```javascript showLineNumbers
// frontend/src/components/Scenario.jsx

import { CameraControls, Environment } from '@react-three/drei'
import { useEffect, useRef } from 'react'

export const Scenario = () => {
  const cameraControls = useRef()
  useEffect(() => {
    cameraControls.current.setLookAt(0, 2.2, 5, 0, 1.0, 0, true)
  }, [])
  return (
    <>
      <CameraControls ref={cameraControls} />
      <Environment preset="sunset" />
    </>
  )
}
```

In the `Scenario` component, we have utilized some elements provided by the `@react-three/drei` library, a set of helpful abstractions for `react-three-fiber`. Below are the elements used in the `Scenario` component:

- **`<CameraControls>`**: This component handles camera controls, allowing users to interact with the 3D view. For example, the user could orbit around an object and zoom in or out.

- **`<Environment>`**: This component sets the scene's environment. By using the `sunset` preset, it automatically adds lighting and a background that simulates a sunset. The `sunset` preset helps improve the visual appearance of the scene without the need to manually configure lights and the background. .

The `Scenario` component returns a JSX fragment containing these three elements. When rendered, this component sets up
camera controls, defines the environment, and adds contact shadows to the 3D scene. It also prepares the scene for other
3D components (such as models, additional lights, etc.) to be added and presented with an already established basic
appearance and controls.

## The Avatar

<ImageBox
  src="/images/ai-avatars/fig2v2.png"
  alt="The avatar editor of Ready Player Me"
  width="728px"
  height="368px"
>
  **Figura 2**. The avatar editor of Ready Player Me.
</ImageBox>

The next step is to obtain an avatar to integrate it into our `Scenario` component. We will access the page [Ready Player Me](https://readyplayer.me/) and create our avatar to do this. No registration is required.

On this site, we can customize our avatars by adjusting gender, skin tone, accessories, hair style and colour, face shape, eye shape, and clothing.

Once your avatar is ready, we copy the avatar link and modify it as follows, adding the query parameters `morphTargets=ARKit, Oculus Visemes`:

```javascript
https://models.readyplayer.me/YOUR_AVATAR_ID.glb?morphTargets=ARKit,Oculus Visemes
```

By adding the query parameters, we can download the .glb file with the attributes to animate the avatar's mouth, eyes, and overall face. [Here](https://docs.readyplayer.me/ready-player-me/api-reference/rest-api/avatars/get-3d-avatars#morphtargets), you can consult the rest of the query parameters that we can use. Once our avatar is downloaded, we place it in the `public/model` directory with a name of your choice. In my case, I have called it `avatar.glb`.

With the `avatar.glb` file, we go to [gltf.pmnd.rs](https://gltf.pmnd.rs/) to create our React component for our avatar.
If you decide to use TypeScript, it is necessary to specify that you also need the corresponding types.

<ImageBox
  src="/images/ai-avatars/fig3v2.png"
  alt="GLTF editor to generate React.js components"
  width="728px"
  height="368px"
>
  **Figura 3**. GLTF editor to generate React.js components.
</ImageBox>

Since we're not using TypeScript in our case, the generated code for our avatar would be as follows:

```javascript showLineNumbers
// frontend/src/components/Avatar.jsx

import React, { useRef } from 'react'
import { useGLTF } from '@react-three/drei'

export function Avatar(props) {
  const { nodes, materials } = useGLTF('/models/avatar.glb')
  return (
    <group {...props} dispose={null}>
      <primitive object={nodes.Hips} />
      <skinnedMesh
        name="EyeLeft"
        geometry={nodes.EyeLeft.geometry}
        material={materials.Wolf3D_Eye}
        skeleton={nodes.EyeLeft.skeleton}
        morphTargetDictionary={nodes.EyeLeft.morphTargetDictionary}
        morphTargetInfluences={nodes.EyeLeft.morphTargetInfluences}
      />
      <skinnedMesh
        name="EyeRight"
        geometry={nodes.EyeRight.geometry}
        material={materials.Wolf3D_Eye}
        skeleton={nodes.EyeRight.skeleton}
        morphTargetDictionary={nodes.EyeRight.morphTargetDictionary}
        morphTargetInfluences={nodes.EyeRight.morphTargetInfluences}
      />
      <skinnedMesh
        name="Wolf3D_Head"
        geometry={nodes.Wolf3D_Head.geometry}
        material={materials.Wolf3D_Skin}
        skeleton={nodes.Wolf3D_Head.skeleton}
        morphTargetDictionary={nodes.Wolf3D_Head.morphTargetDictionary}
        morphTargetInfluences={nodes.Wolf3D_Head.morphTargetInfluences}
      />
      <skinnedMesh
        name="Wolf3D_Teeth"
        geometry={nodes.Wolf3D_Teeth.geometry}
        material={materials.Wolf3D_Teeth}
        skeleton={nodes.Wolf3D_Teeth.skeleton}
        morphTargetDictionary={nodes.Wolf3D_Teeth.morphTargetDictionary}
        morphTargetInfluences={nodes.Wolf3D_Teeth.morphTargetInfluences}
      />
      <skinnedMesh
        geometry={nodes.Wolf3D_Glasses.geometry}
        material={materials.Wolf3D_Glasses}
        skeleton={nodes.Wolf3D_Glasses.skeleton}
      />
      <skinnedMesh
        geometry={nodes.Wolf3D_Headwear.geometry}
        material={materials.Wolf3D_Headwear}
        skeleton={nodes.Wolf3D_Headwear.skeleton}
      />
      <skinnedMesh
        geometry={nodes.Wolf3D_Body.geometry}
        material={materials.Wolf3D_Body}
        skeleton={nodes.Wolf3D_Body.skeleton}
      />
      <skinnedMesh
        geometry={nodes.Wolf3D_Outfit_Bottom.geometry}
        material={materials.Wolf3D_Outfit_Bottom}
        skeleton={nodes.Wolf3D_Outfit_Bottom.skeleton}
      />
      <skinnedMesh
        geometry={nodes.Wolf3D_Outfit_Footwear.geometry}
        material={materials.Wolf3D_Outfit_Footwear}
        skeleton={nodes.Wolf3D_Outfit_Footwear.skeleton}
      />
      <skinnedMesh
        geometry={nodes.Wolf3D_Outfit_Top.geometry}
        material={materials.Wolf3D_Outfit_Top}
        skeleton={nodes.Wolf3D_Outfit_Top.skeleton}
      />
    </group>
  )
}

useGLTF.preload('/models/avatar.glb')
```

The previous code contains a React component called `Avatar`, which loads and renders the 3D model `avatar.glb` using `Three.js` through the `@react-three/drei` library. This component uses the `useGLTF` hook to load a GLB file, a binary format of glTF (GL Transmission Format), a standard specification for the efficient distribution of 3D models.

The previous component does the following:

1. **Loading the 3D Model:** The `useGLTF` hook loads the 3D model from the provided path ("models/avatar.glb"). Once loaded, this hook returns two important objects: `nodes`, which contain all the nodes or elements of the model, and `materials`, which store the materials defined in the 3D model.

[^1]: See [UneeQ](https://www.digitalhumans.com/blog/how-chatgpt-bard-and-other-llms-are-signaling-an-evolution-for-ai-digital-humans), [Wikipedia](https://en.wikipedia.org/wiki/Virtual_human), [Quantum Capture](https://www.quantumcapture.com/insights/meetquantum-gte8t), [Unreal Engine](https://www.unrealengine.com/en-US/explainers/digital-humans/how-do-you-create-a-digital-human), [ATRYTONE](https://atrytone.com/digital-humans-virtual-humans-differences-overview/), [Synthesia](https://www.synthesia.io/post/digital-humans) for more information on Digital Humans or AI Avatars.
[^2]: See [How ChatGPT, Bard and other LLMs are signalling an evolution for AI digital humans](https://www.digitalhumans.com/blog/how-chatgpt-bard-and-other-llms-are-signaling-an-evolution-for-ai-digital-humans).
