---
title: 'Redes neuronales representadas como DAGs de programas computacionales parametrizados'
date: '2024/06/23'
tags: ['matemáticas', 'ciencias de la computación']
draft: false.
time: '15 min'
summary: 'Las redes neuronales se pueden ver como algoritmos con parámetros ajustables, optimizados mediante aprendizaje automatizado. Este enfoque nos permite abordarlas desde una perspectiva matemática y computacional más amplia. En este artículo, exploraremos cómo representar estos algoritmos como grafos dirigidos acíclicos (DAG).'
bibliography: references-data.bib
canonicalUrl: 'https://www.asanchezyali.com/blog/es/number-theory/20230108DivisionAlgorithm/'
headerImage: '/images/differentiable-programs/header.avif'
images: ['/images/differentiable-programs/header.avif']
language: 'es'
---

Las redes neuronales se pueden ver como algoritmos con parámetros ajustables, optimizados mediante aprendizaje automatizado. Este enfoque nos permite abordarlas desde una perspectiva matemática y computacional más amplia. En este artículo, exploraremos cómo representar estos algoritmos como grafos dirigidos acíclicos (DAG).

La representación de redes neuronales como DAGs tiene varias ventajas:

1. **Visualización**: Los DAGs ofrecen una forma clara de entender la estructura de un programa.
2. **Optimización**: Los DAGs permiten optimizar programas mediante técnicas de optimización de grafos.
3. **Diferenciación automática**: Facilitan la implementación de algoritmos como la retropropagación.
4. **Paralelización**: Los DAGs permiten ejecutar programas en paralelo en hardware especializado como GPUs y TPUs.

Este artículo examinará cómo los DAGs mejoran la comprensión y diseño de redes neuronales complejas y su relación con técnicas avanzadas de aprendizaje profundo, como las redes residuales y recurrentes.

## Programas computacionales parametrizados

### Cadenas de computación

Para comenzar, consideremos **programas computacionales** simples definidos por una **secuencia** de funciones $f_1, \ldots, f_n$ que se aplican
de forma secuencial a una entrada
$s_0 \in S_{0}.$ Denominaremos a estos programas **cadenas de computación** y los representaremos como $f = f_n \circ \cdots
\circ f_1.$

Un ejemplo práctico de una cadena de computación es el procesamiento de imágenes, donde una imagen puede someterse a una
serie de transformaciones como recorte,
rotación y normalización. En el contexto de las redes neuronales, estas transformaciones suelen estar parametrizadas y
sus
parámetros se ajustan mediante un proceso de optimización durante el entrenamiento.

<ImageBox
  src="/images/differentiable-programs/figure1.svg"
  alt="Computation Chain"
  width="750px"
  height="150px"
>
  **Figura 1**. Una cadena de computación es una secuencia de composiciones de funciones. En el
  gráfico anterior, cada nodo intermedio representa una función. La primera flecha representa la
  entrada, y la última la salida. Las flechas interiores representan las dependencias de las
  funciones con respecto a las salidas previas o el entrada inicial.
</ImageBox>

Formalmente, una cadena de computación puede ser escrita como:

$$
\begin{equation}
\begin{aligned}
s_0 &\in S_{0}, \\
s_{1} &= f_{1}(s_{0}) \in S_{1}, \\
s_{2} &= f_{2}(s_{1}) \in S_{2}, \\
&\vdots \\
s_{n} &= f_{n}(s_{n-1}) = f(s_0) \in S_{n}.\\
\end{aligned}
\end{equation}
$$

Donde $s_0$ es la **entrada**, $s_k\in S_k$ para $k = 1, \ldots,
 n-1,$ son los **estados intermedios** del programa y $s_n\in S_n$ es la
**salida**. Por supuesto, el dominio (espacios de entrada) de $f_k$ debe ser compatible con la imagen (espacio de
salida) de $f_{k-1}$ para que la composición tenga sentido. Es decir, que $f_k: S_{k-1} \to S_k$ para $k = 1, \ldots,
 n.$ De manera equivalente a la ecuación (1), podemos escribir la cadena de computación como:

$$
\begin{equation}
s_n = f(s_0) = f_n \circ \cdots \circ f_1(s_0).
\end{equation}
$$

Una cadena de computación puede ser representada como un **grafo dirigido acíclico (DAG)**, donde los nodos representan
las funciones y las aristas representan las dependencias entre las funciones.

### Grafos acíclicos dirigidos

En un programa genérico, las funciones intermedias pueden depender de las salidas de otras funciones. Tales dependencias
pueden ser representadas como un grafo dirigido acíclico (DAG). Un **grafo dirigido** $G = (V, E)$ consiste de un
conjunto de nodos $V$ y un conjunto de aristas $E\subseteq V \times V.$ Un arista $(u, v) \in E$ es un par ordenad de
vertices $u, v\in V$. Es también representado como $u \to v.$, para indicar que $u$ depende de $v.$ Para representar las
entradas y las salidas, será conveniente usar **semi-aristas entrantes** $\to j$ y **semi-aristas salientes** $i \to.$

En un grafo $G = (V, E)$, los **padres** de un nodo $v\in V$ son los nodos $u\in V$ tales que $(u, v)\in E,$ denotado
por

$$
\begin{equation}
\text{padres}(v) = \{u\in V: (u, v)\in E\}.
\end{equation}
$$

De manera similar, los **hijos** de un nodo $v\in V$ son los nodos $w\in V$ tales que $(v, w)\in E,$ denotado por

$$
\begin{equation}
\text{hijos}(v) = \{w\in V: (v, w)\in E\}.
\end{equation}
$$

Los vertices sin padres son llamados **raíces** y los vertices sin hijos son llamados **hojas**.

Un **camino** de $i$ a $j$ en un grafo $G = (V, E)$ es una secuencia de nodos $v_1, \ldots, v_k$ tal que $i \to v_1 \to
\cdots \to v_k \to j.$ Un camino es **simple** si no contiene nodos repetidos. Un grafo es **acíclico** si no contiene
ciclos, es decir, no hay caminos simples de un nodo a sí mismo. Un grafo acíclico dirigido (DAG) es un grafo dirigido
que es acíclico.

La aristas de un DAG inducen un **orden parcial** en los nodos, que denotaremos por $i \preceq j$ si existe un camino de
$i$ a $j.$ Un orden parcial es una relación reflexiva, transitiva y antisimétrica. El orden es parcial porque no todos
los nodos están relacionados entre sí. No obstante, podemos definir un **orden total** llamado **orden topológico**:
cualquier orden tal que $i \preceq j$ si y solo si no hay un camino de $j$ a $i.$

### Programas computacionales como DAGs

Nosotros que un programa computacional es una función valida en el sentido matemático, es decir, el programa debería
retornar valores idénticos para las mismas entradas y no debería tener efectos secundarios. También asumimos que el
programa se detiene en un número finito de pasos. Por lo tanto, un programa puede estar hecho de un número finito de
funciones intermedias y variables intermedias, de tal forma que las dependencias entres las funciones y las variables
pueden ser representados como un grafo dirigido acíclico. Sin perdida de generalidad, podemos hacer las siguientes
suposiciones:

1. Existe un nodo **entrada** $s_0$ y un nodo **salida** $s_n.$
2. Cada función intermedia $f_k$ produce un solo valor $s_k \in S_k.$

Con un número finito de nodos como $V = \{0, 1, \ldots, n\}$. El node $0$ es una raíz, correspondiente a la
entrada del programa, y el nodo $n$ es una hoja, correspondiente a la salida del programa. Las funciones intermedias.
Debido al tercer supuesto, aparte de $s_0$, cada variable $s_k$ está en biyección con una función $f_k.$ Por lo tanto,
el node $0$ representa la entrada $s_0$, y los nodos $1, \ldots, n$ representan las funciones $f_1, \ldots,
f_n$ y salidas $s_1, \ldots, s_n,$ es decir un nodo $k$ representa la función $f_k$ y la salida $s_k.$

Las aristas de un DAG representan las dependencias. Los padres $\{i_1, \ldots, i_{p_k}\} := \text{padres}(k)$ de un nodo
$k,$ donde $p_k:=|\text{padres}(k)|,$ indican las variables $s_{\text{padres}(k)} := \{s_{i_1}, \ldots, s_{i_{p_k}}\}$
que la función $f_k$ necesita para computar su salida $s_k$. Dicho de otro modo, los padres $\{i_1, \ldots, i_{p_k}\}$
indican funciones $f_{i_1}, \ldots, f_{i_{p_k}}$ que son necesarias para computar $f_k.$

<PseudoCode title="Algoritmo 1. Ejecución de un programa">
  <PseudoCodeLine showNumber={false} indent={0}>
    $\text{\textbf{Funciones:}}$ $f_1, \ldots, f_n \text{ en orden topológico}$
  </PseudoCodeLine>
  <PseudoCodeLine showNumber={false} indent={0}>
    $\text{\textbf{Entrada:}}$ $s_0\in S_0$
  </PseudoCodeLine>
  <PseudoCodeLine number={1}>
    $\text{\textbf{Para} } k = 1, \ldots, n$ $\text{\textbf{hacer}}$
  </PseudoCodeLine>
  <PseudoCodeLine number={2} indent={2}>
    <span>Encontrar los nodos padres</span> $\{i_1, \ldots, i_{p_k}\} := \text{padres}(k)$
  </PseudoCodeLine>
  <PseudoCodeLine number={3} indent={2}>
    <span>Calcular</span> $s_k :=f_k\big(s_{padres(k)}\big):= f_k(s_{i_1}, \ldots, s_{i_{p_k}})$
  </PseudoCodeLine>
  <PseudoCodeLine howNumber={false} indent={0}>
    $\text{\textbf{Fin:} } f(s_0) = s_n$
  </PseudoCodeLine>
</PseudoCode>

#### Ejecución de un programa

Para ejecutar un programa, nosotros necesitamos asegurar que las funciones intermedias son evaluadas en un orden en el
orden correcto. Por lo tanto, nosotros asumimos que los nodos $V=\{0, 1, \ldots, n\}$ están ordenados en un **orden
topológico**, (si este no es el caso, entonces el programa no puede ser ejecutado). Nosotros podemos ejecutar un
programa para evaluar la salida $s_k \in [n]$

$$
\begin{equation}
s_k := f_k\big(s_{\text{padres}(k)}\big):=f_k(s_{i_1}, \ldots, s_{i_{p_k}})\in S_k.
\end{equation}
$$

Obsérvese que podemos ver $f_k$ como una función de una sola entrada de $s_{\text{padres}(k)},$ el cual es una tupla de
elementos, o como una función de múltiples entradas $s_{i_1}, \ldots, s_{i_{p_k}}.$ Los dos puntos de vistas son
esencialmente equivalentes. El proceso de ejecución de un programa es descrito en el Algoritmo 1.

#### Tratamiento de múltiples entradas o salidas del programa

Cuando un programa tiene múltiples entradas, los agrupamos siempre en una sola entrada $s_0 \in S_0.$ como $s_0 = (s_{0,
1}, \ldots, s_{0, m_0})$ con $S_0 = S_{0, 1} \times \cdots \times S_{0, m_0},$ ya que las funciones posteriores siempre
pueden filtrar los elementos de $s_0$ que necesitan. Del mismo modo, si una función intermedia $f_k$ tiene varias
salidas, siempre las agrupamos en una sola salida $s_k \in S_k$ como $s_k = (s_{k, 1}, \ldots, s_{k, m_k})$ con $S_k =
S_{k, 1} \times \cdots \times S_{k, m_k},$ ya que las funciones posteriores siempre pueden filtrar los elementos de
$s_k$.

<ImageBox
  src="/images/differentiable-programs/figure2.svg"
  alt="Computation Chain"
  width="750px"
  height="150px"
>
  **Figura 2**. Dos posibles representaciones de un programa. **Izquierda:** La funciones son
  representadas como nodos y las dependencias como aristas. **Derecha:** Las funciones son
  representadas como nodos y las dependencias como un conjunto de nodos disjuntos. En ambos casos,
  la entrada es representada por un nodo raíz y la salida por un nodo hoja.
</ImageBox>

#### Representación alternativa de programas: grafos bipartitos

En nuestro formalismo, dado que una función $f_k$ siempre tiene una única salida $s_k,$ se puede considerar que un nodo
$k$ representa tanto la función $f_k$ como la salida $s_k.$ Sin embargo, también es posible considerar dos conjuntos de
nodos disjuntos: uno para las funciones y otro para las salidas, es decir, un grafo bipartito. Este formalimso es
similar a los grafos factoriales (_Frey et al_.,
[1997](https://people.ee.ethz.ch/~loeliger/localpapers/FG_Allerton1997.pdf);
_Loeliger_,
[2004](https://ieeexplore.ieee.org/document/1267047))
utilizados en la modelización probabilísticas, pero con aristas dirigidas. Una ventaja de este formalismo es que permite
que las funciones tengan múltiples salidas. Por simplicidad, nos centraremos en la representación de un solo conjunto de nodos.

### Circuitos aritméticos

Los circuitos aritméticos son uno de los ejemplos más sencillos de computación grafo, originarios de **la teoría de la
complejidad computacional**. Formalmente, un circuito aritmético sobre un campo $\mathbb{F}$, como por ejemplo los
número reales $\mathbb{R}$ o los números complejos $\mathbb{C},$ es un grafo dirigido acíclico (DAG) donde los nodos
raíces son elementos del campo $\mathbb{F}$ y las funciones intermedias son operaciones aritmética como la suma o la
multiplicación. Estas últimas suelen denominarse **compuertas aritméticas**. Contrariamente al caso general de grafos
computacionales, cada función sea una suma o un producto puede tener múltiples nodos raíces. Los nodos raíces pueden ser
variables o constantes, y deben pertenecer al campo $\mathbb{F}.$ Los circuitos aritméticos puede utilizarse para
calcular polinomios. Puede existir múltiples circuitos aritméticos para representar un polinomio dado. Para comparar dos
circuitos aritméticos que representan el mismo polinomio, una noción intuitiva de la **complejidad** es el **tamaño del
circuito**, como se define a continuación.

<MathBox title="Definición 1. Circuito y tamaño polinomial" variant="definition">
  El **tamaño** $S(C)$ de un circuito aritmético $C$ es el número de aristas en el grafo dirigido
  acíclico (DAG) que representa al circuito. EL **tamaño del polinomio** $S(f)$ de un polinomio $f$
  es el tamaño del circuito aritmético $C$ más pequeño que representa a $f.$
</MathBox>

Para más información sobre circuitos aritméticos, se puede consultar el libro de _Chen et al_.
[2011](https://www.math.ias.edu/~avi/PUBLICATIONS/ChenKaWi2011.pdf).

## Redes de alimentación hacia adelante

Una red de alimentación hacia adelante (_feedforward network_) es un tipo de cadena de computación con funciones parametrizadas $f_1, \ldots, f_n$ que se aplican de forma secuencial a una
entrada $s_0 \in S_0.$ En este caso, las funciones $f_k$ son usualmente funciones parametrizadas que dependen de un
vector de parámetros $w_k \in \mathcal{W}_k,$ donde $\mathcal{W}_k$ es el espacio de parámetros de la función $f_k.$ Por lo
tanto,

$$
\begin{equation}
\begin{aligned}
x &= s_0 \in S_{0}, \\
s_{1} &= f_{1}(s_{0}; w_1) \in S_{1}, \\
s_{2} &= f_{2}(s_{1}; w_2) \in S_{2}, \\
&\vdots \\
s_{n} &= f_{n}(s_{n-1}; w_n) = f(s_0; w) \in S_{n}.\\
\end{aligned}
\end{equation}
$$

para una entrada $s_0\in S_0$ y para los **parámetros entrenables** $w = (w_1, \ldots, w_n) \in \mathcal{W} =
\mathcal{W}_1 \times \cdots \times \mathcal{W}_n.$ Cada función $f_k$ es una **capa** de la red y cada $s_k \in S_k$ es
una **representación intermedia** de la entrada $s_0.$ la dimension de $S_k$ se conoce como la **dimensión de la capa**
(o número de unidades ocultas) de la capa $k.$ Una red de alimentación hacia adelante puede ser definida como una
función $f:S_0 \times \mathcal{W} \to S_n$ que mapea una entrada $s_0$ y parámetros $w$ a una salida $s_n.$

Dado un programa parametrizado de este tipo, los parametros $w$ podemos aprender los parámetros ajustando $w$
de acuerdo a un conjunto de datos de entrenamiento. Por ejemplo, dado un conjunto de datos de pares $(x_i, y_i),$ podemos
minimizar la función de pérdida $L(w) = \sum_{i=1}^{m} \ell(f(x_i; w), y_i).$ La minimización de la función de
pérdida se puede hacer utilizando un algoritmo de optimización como el descenso de gradiente.

## Perceptrones multicapa

### Combinación de capas afines y funciones de activación

En la sección anterior, no especificamos cual es la forma de parametrizar las redes de alimentación hacia adelante. Una
parametrización típica, es el perceptron multicapa (_multilayer perceptron_ o MLP), que utiliza capas totalmente
conectadas (también conocidas densas) de la forma

$$
\begin{equation}
s_k = f_k(s_{k-1}; w_k) = a_k(W_k s_{k-1} + b_k),
\end{equation}
$$

donde $w_k = (W_k, b_k)$ son los parámetros de la capa $k,$ $W_k$ es un matriz de pesos y $b_k$ es un vector de sesgos.
Ademas la capa las podemos descompones en dos funciones. La función $s \mapsto W_k s + b_k$ es una **capa afín** y la
función $s \mapsto a_k(s)$ es no lineal sin parámetros, llamada **función de activación**.

Generalmente, podemos remplazar el producto matrix-vector $W_k s_{k-1}$ por cualquier función lineal de $s_{k-1}.$ Por
ejemplo, las capas convolucionales utilizan de convolución de la entrada $s_{k-1}$ con algunos filtros $W_k.$

<MathBox title="Remark 1. Tratamiento de entradas múltiples" variant="remark">
  A veces es necesario tratar con redes de múltiples entradas. POr ejemplo, supongamos que queremos
  diseñar una función $g(x_1, x_2, w_g)$, donde $x_1 \in \mathcal{X}_1$ y $x_2 \in \mathcal{X}_2$
  son dos entradas. Una forma simple de hacerlo es usar la concatenación $x = x_1 \oplus x_2 \in
  \mathcal{X}_1 \oplus \mathcal{X}_2$ como entrada a una red $f(x, W_f).$ Alternativamente, en lugar
  de concatenar las entradas, se pueden concatenar después de haber pasado por una o más capas
  ocultas.
</MathBox>

### Relación con los modelos lineales generalizados

Cuando la profundidad de la red es $n=1$ (es decir, una sola capa), la salida un MLP es una función lineal de la entrada
$x$ es

$$
\begin{equation}
s_1 = a_1(W_1 x + b_1).
\end{equation}
$$

Esto es llamado un **modelo lineal generalizado** (_generalized linear model_ o GLM). En este caso, la función de
activación $a_1$ es la función de identidad. Los modelos lineales generalizados son un caso especial de los MLPs. Es
particular, cuando $a_1$ es la función $\operatorname*{softargmax}$, se tiene un modelo de regresión logística. Por lo
general para la profundidad $n>1,$ la salida de un MLP es

$$
\begin{equation}
s_n = a_n(W_n s_{n-1} + b_n).
\end{equation}
$$

Esto puede ser visto como un GLM sobre la **representación intermedia** $s_{n-1}$ de la entrada $s_0.$ Este es el
principal atractivo de los MLPs: la capacidad de aprender representaciones intermedias que son útiles para la tarea de
interés. Veremos que los MLPs también pueden utilizarse como subcomponentes en otras arquitecturas.

## Funciones de activación

Como se mencionó anteriormente, la redes de alimentación hacia adelante utilizan funciones de activación $a_k$ para cada
capa. En esta sección, vamos a describir algunas de las funciones de activación más comunes de escalar a escalar o de
vector a escalar. También presentaremos algunas funciones de probabilidad que pueden ser utilizadas como funciones de
activación.

### Funciones de activación de escalares a escalares nolineales

La función **ReLU** (_Rectified Linear Unit_) es una función de activación no lineal que se define como la parte no
negativa de su argumento. Es decir, la función ReLU es la función identidad en los valores no negativos y cero en los
valores negativos:

$$
\begin{equation}
\text{ReLU}(x) = \max(0, x) = \begin{cases}
x & \text{si } x \geq 0, \\
0 & \text{si } x < 0.
\end{cases}
\end{equation}
$$

Es una función lineal por partes e incluye un punto de inflexión en $x=0.$ Una perceptron multicapa con activaciones
ReLU se denomina red neuronal rectificadora. Las capas toman las forma

$$
\begin{equation}
s_k = \text{ReLU}(W_k s_{k-1} + b_k).
\end{equation}
$$

donde la ReLU es aplicada elemento a elemento. La función ReLU puede ser remplazada con una versión suavizada, conocida
como **softplus**:

$$
\begin{equation}
\text{softplus}(x) = \log(1 + \exp(x)).
\end{equation}
$$

A diferencia de la ReLU, la softplus es una función suave, diferenciable y estrictamente positiva.

### Funciones de activación de vectores a escalares nolineales

A menudo es útil reducir vectores a escalares. Estos valores escalares pueden ser vistos como puntajes o probabilidades,
que representan la importancia de un vector de entrada. Una función común para hacer esto es usando el valor máximo,
también conocido como **max-pooling**. Dado un vector $x\in \mathbb{R}^d,$ la función de max-pooling es

$$
\begin{equation}
\text{maxpooling}(x) = \max_{j \in [\,d\,]} x_j.
\end{equation}
$$

Como una aproximación suave de la función de max-pooling, se puede usar la función **log-sum-exp**, la cual se comporta
como una versión suavizada de la función max-pooling:

$$
\begin{equation}
\text{logsumexp}(x) := \text{softmax}(x) := \log\sum_{j=1}^{d} \exp(x_j).
\end{equation}
$$

La función log-sum-exp puede ser vista como una generalización de la función **softplus** a vectores, en efecto para
todo $x \in \mathbb{R}$

$$
\begin{equation}
\operatorname*{logsumexp}((x, 0)) = \operatorname*{softplus}(x).
\end{equation}
$$

Una implementación númericamente estable de la función log-sum-exp es la siguiente:

$$
\begin{equation}
\text{logsumexp}(x) = \text{logsumexp}(x - c\mathbf{1}) + c,
\end{equation}
$$

donde $\mathbf{1}$ es un vector de unos y $c = \max_{j\in [\,d\,]} x_j.$

### Mapeos des probabilidad de escalar a escalar

En algunas ocasiones queremos mapear algún número real en un número entre 0 y 1, que puede representar la probabilidad
de un evento. Para se propósito se usa con frecuencia las **sigmoides**. Una sigmoide es una función cuya curva se
caracteriza por tener forma de «S». Estas funciones se utilizan para comprimir valores reales en un intervalo. Un
ejemplo es la función paso binaria, también conocida como la función de paso de Heaviside,

$$
\begin{equation}
\text{step}(x) = \begin{cases}
1 & \text{si } x \geq 0, \\
0 & \text{si } x < 0.
\end{cases}
\end{equation}
$$

Es un mapeo de $\mathbb{R}$ a $\{0, 1\}.$ Desafortunadamente, la función de paso binaria no es discontinua en $x=0.$
Además, debido a que la función es constante en todos los demás puntos, tiene derivada cero en estos puntos, lo que
dificulta su uso como parte de una red neuronal con retropropagación.

Una mejor sigmoide es la **función logítica**, la cual mapea de $\mathbb{R}$ a $(0, 1)$ y es definida como:

$$
\begin{equation}
\begin{aligned}
\text{logistic}(x) &= \frac{1}{1 + \exp(-x)} \\
&= \frac{\exp(x)}{1 + \exp(x)} \\
&= \frac{1}{2} + \frac{1}{2}\tanh\left(\frac{x}{2}\right).
\end{aligned}
\end{equation}
$$

Ella mapea $(-\infty, 0)$ a $(0, 0.5)$ y $\lbrack 0, \infty \rparen$ a $\lbrack 0.5, 1\rparen.$ y satisface que $\text{logistic}(0) = 0.5.$ Por lo
tanto puede ser vista como una función de probabilidad. La función logística puede ser vista como una versión suavizada
de la función de paso $\text{step}(x).$ Además la función logística se puede obtener como la derivada de la función
$\text{softplus}(x),$ es decir, para todo $x \in \mathbb{R}$

$$
\begin{equation}
\text{logistic}(x) = \frac{d}{dx}\text{softplus}(x).
\end{equation}
$$

Dos propiedades importantes de la función logística es que para todo $x \in \mathbb{R}$

$$
\begin{equation}
\text{logistic}(-x) = 1 - \text{logistic}(x),
\end{equation}
$$

y

$$
\begin{equation}
\begin{aligned}
\frac{d}{dx}\text{logistic}(x) &= \text{logistic}(x)(1 - \text{logistic}(x)) \\
&= \text{logistic}(x)\text{logistic}(-x).
\end{aligned}
\end{equation}
$$

### Mapeos de probabilidad de vectores a vectores

En algunas ocasiones queremos mapear un vector en un vector de probabilidades. Este es una mapero de $\mathbb{R}^d$ a un
simplex de probabilidad, definido por:

$$
\begin{equation}
\Delta^{d} = \left\{p\in \mathbb{R}^d: \forall j \in [\,d\,], p_j \geq 0 \text{ y } \sum_{j=1}^{d} p_j = 1\right\}.
\end{equation}
$$

Dos ejemplos de funciones de mapeo de probabilidad de vector a vector son la función **$\text{argmax}$ y la función
**$\text{softargmax}.$ La función $\text{argmax}$ mapea un vector a un vector de probabilidad que es cero en todas las
entradas excepto en la entrada con el valor máximo que lo hace uno. En efecto,

$$
\begin{equation}
\text{argmax}(x) = \phi(\arg\max_{j\in [\,d\,]} x_j),
\end{equation}
$$

donde $\phi(j)$ denota el _one-hot encoding_ de un entero $j\in [\,d\,],$ es decir, $\phi(j)_i = 1$ si $i = j$ y cero en
otro caso, esto es

$$
\begin{equation}
\phi(j) = (0, \ldots, 0, 1, 0, \ldots, 0) = e_j \in \{0, 1\}^d.
\end{equation}
$$

Este mapeo coloca toda la
masa de probabilidad en la entrada con el valor máximo, en caso de empate, se selecciona la primera entrada con el valor
máximo. Desafortunadamente, la función $\text{argmax}$ no es diferenciable, lo que dificulta su uso en la
retropropagación.

Una versión suavizada de la función $\text{argmax}$ es la función \*\*$\text{softargmax}$. La función $\text{softargmax}$
está definida como

$$
\begin{equation}
\text{softargmax}(x) = \frac{\exp(x)}{\sum_{j=1}^{d} \exp(x_j)}.
\end{equation}
$$

Este operador es comúnmente conocido en la literatura como la función _softmax_, pero esta denominación es errónea: este
operadora realmente define una versión suavizada de la función $\text{argmax}.$ La salida de $\text{softargmax}$
pertenece al interior relativo del simplex de probabilidad $\Delta^{d},$ lo que significa que nunca puede alcanzar los
bordes del simplex. Si denotamos $\pi = \text{softargmax}(x),$ entonces $\pi_j \in (0, 1),$ es decir $0 < \pi_j < 1$
para todo $j \in [\,d\,].$ La función $\text{softargmax}$ es el gradiente de la función $\text{log-sum-exp}$, es decir,
para todo $x \in \mathbb{R}^d$

$$
\begin{equation}
\text{softargmax}(x) = \nabla \text{log-sum-exp}(x).
\end{equation}
$$

La función $\text{softargmax}$ se puede ver como una generalización de la función $\text{logistic}$ a vectores, en
efecto

$$
\begin{equation}
\text{softargmax}(x) = \begin{bmatrix}
\text{logistic}(x_1) \\
\vdots \\
\text{logistic}(x_d)
\end{bmatrix}.
\end{equation}
$$

<MathBox title="Remark 2. Grados de libertad e inversa de la función softargmax" variant="remark">
  La función $\text{softargmax}$ satisface la propiedad que para todo $x \in \mathbb{R}^d$ y para todo $c \in
  \mathbb{R},$
  
  $$
  \begin{equation}
  \pi = \text{softargmax}(x + c\mathbf{1}) = \text{softargmax}(x).
  \end{equation}
  $$
  Esto quiere decir que la función $\text{softargmax}$ tiene $d-1$ grados de libertad y que no es invertible. Sin
  embargo, debido a la anterior propiedad, sin perdida de generalidad, nosotros podemos imponer que $\pmb{x}^\top
  \pmb{1} = \sum_{j=1}^{d} x_j = 0$ (si no es el caso, podemos simplemente hacer $\pmb{x} \leftarrow x_i - \bar{x}$, en
  donde $\bar{x} = \frac{1}{d}\sum_{j=1}^{d} x_j$ es la media de $x$). Con esta restricción y junto con el hecho de

$$
\begin{equation}
\log(\pi_i) = x_i - \log\sum_{j=1}^{d} \exp(x_j),
\end{equation}
$$

obtenemos

$$
\begin{equation}
\sum_{j=1}^{d} \log(\pi_j) = - d \log \sum_{j=1}^{d} \exp(x_j).
\end{equation}
$$

Por lo tanto, la función $\text{softargmax}$ es invertible en el sentido de que podemos recuperar $x$ a partir de

$$
\begin{equation}
x_i = [\text{softargmax}^{-1}(\pi)]_i = \log(\pi_i) - \frac{1}{d}\sum_{j=1}^{d} \log(\pi_j).
\end{equation}
$$

</MathBox>

## Redes neuronales residuales

Como se mencionó anteriormente, las redes de alimentación hacia adelante son un tipo de cadena de computación. En este
caso, las funciones intermedias son funciones parametrizadas que se aplican de forma secuencial a una entrada. En
efecto, consideremos una red de alimentación hacia adelante con $n+1$ capas $f_1, \ldots, f_{n+1}$. Seguramente, siempre
que $f_{n+1}$ sea la función identidad, el conjunto de funciones que esta red puede representar es el mismo que el de
una red de $n$ capas $f_1, \ldots, f_n.$ Por lo tanto, podemos considerar una red de $n+1$ capas como una red de $n$. En
otras palabras,la profundidad, en teoría, no debería perjudicar el poder expresivo de las redes de alimentación hacia
adelante. Desafortunadamente, la suposición de que cada $f_k$ es una función identidad no es realista. Esto significa
que la redes más profundas a veces pueden ser más difíciles de entrenar que las más superficiales, haciendo que la
precisión se sature o degrade en función de la profundidad.

La idea clave de las redes neuronales residuales (_residual neural networks_ o ResNets) (_He et al._,
[2016](https://arxiv.org/pdf/1512.03385)) es introducir **bloques residuales** entre las capas, de tal forma que la salida
de una capa se convierte en la entrada de una capa posterior. Formalmente, un bloque residual es una función de la forma

$$
\begin{equation}
s_{k} = f_k(s_{k-1}; w_k):= s_{k-1} + h_k(s_{k-1}; w_k).
\end{equation}
$$

La función $h_k$ es la **función residual**, dado que modela la diferencia entre la entrada y la salida, es decir,
$h_k(s_{k-1}; w_k) = s_{k} - s_{k-1}.$ La función $h_k$ puede ser vista como una **corrección** que se suma a la entrada
$s_{k-1}$ para obtener la salida $s_k.$ La suma con $s_{k-1}$ se conoce como **conexión de salto** (_skip connection_ o
_shortcut_). Siempre que se posible ajusta $w_k$ para que $h_k(s_{k-1}; w_k) = 0,$ entonces la función $f_k$ se
convierte en la función identidad. Por ejemplo, si nosotros usamos:

$$
\begin{equation}
h_k(s_{k-1}; w_k) = C_ka_k(W_ks_{k-1} + b_k) + d_k,
\end{equation}
$$

donde $w_k = (W_k, b_k, C_k, d_k)$ son los parámetros de la capa $k,$ entonces basta con ajustar $C_k = 0$ y $d_k = 0$
para que $h_k(s_{k-1}; w_k) = 0.$ Los bloque residuales son conocidos para remediar el problema de gradiente de fuga.

En muchos artículos y paquetes de software se incluye una activación adicional y en su lugar se define el bloque
residual como:

$$
\begin{equation}
s_{k} = f_k(s_{k-1}; w_k):= a_k(s_{k-1} + h_k(s_{k-1}; w_k)),
\end{equation}
$$

donde $a_k$ es usualmente la función ReLU. Si se debe incluir esta activación adicional o no, es esencialmente una
decisión de modelado. En la práctica, los bloques residuales también pueden incluir operaciones adicionales como
normalización por lotes (batch norm) y capa convolucionales.

## Redes neuronales recurrentes

La redes neuronales recurrentes (_recurrent neural networks_ o RNNs) (_Rumelhart et al._,
[1986](https://www.nature.com/articles/323533a0)) son un tipo de red neuronal que
operan con secuencias de vectores, ya sea como entrada, salida o ambas. Su parametrización real depende de la
configuración, pero la idea central es mantener un vector de estado que se actualiza paso a paso mediante una función
recursiva que utiliza parámetros compartidos a través de los pasos. Distinguimos entre las siguientes configuraciones:

1. Vector a secuencia (uno a muchos):
   $$
   \begin{equation}
   f^d:\mathbb{R}^d \times \mathbb{R}^{p} \to \mathbb{R}^{l\times m }.
   \end{equation}
   $$
2. Secuencia a vector (muchos a uno):
   $$
   \begin{equation}
   f^e:\mathbb{R}^{l\times d} \times \mathbb{R}^{p} \to \mathbb{R}^{m}.
   \end{equation}
   $$
3. Secuencia a secuencia (muchos a muchos, alineados):
   $$
   \begin{equation}
   f^a:\mathbb{R}^{l\times d} \times \mathbb{R}^{p} \to \mathbb{R}^{l\times m}.
   \end{equation}
   $$
4. Secuencia a secuencia (muchos a muchos, no alineados):
   $$
   \begin{equation}
   f^u:\mathbb{R}^{l\times d} \times \mathbb{R}^{p} \to \mathbb{R}^{l'\times m}.
   \end{equation}
   $$

donde $l$ es la longitud de la secuencia de entrada y $l'$ es la longitud de la secuencia de salida. Observe que se usa
el mismo número de parámetros $p$ para cada una de las configuraciones, pero esto por supuesto no es necesario. A lo
largo de este post, usaremos la notación $p_{1:l} := (p_1, \ldots, p_l)$ para denotar una secuencia de $l$ vectores.

### Vector a secuencia

En esta configuración, una función decodificadora $p_{1:l} = f^d(x; w)$ mapea un vector $x \in \mathbb{R}^d$ y un vector de
parámetros $w \in \mathbb{R}^p$ a una secuencia de vectores $p_{1:l} \in \mathbb{R}^{l\times m}.$

<ImageBox
  src="/images/differentiable-programs/RNNv1.svg"
  alt="Computation Chain"
  width="500px"
  height="350px"
>
  **Figura 3**. Uno a muchos (decodificación). Un vector $x \in \mathbb{R}^d$ es mapeado a una secuencia de vectores $p_{1:l} \in \mathbb{R}^{l\times m}.$
</ImageBox>

Esto es útil para la generación de subtítulos de imágenes, donde se genera una oración (una secuencia de palabras) a
partir de una imagen (un vector de píxeles). Formalmente, podemos definir $p_{1:l} = f^d(x; w)$ a través de la
recursión:

$$
\begin{equation}
\begin{aligned}
s_i &:= g(x, s_{i-1}; w_g), \quad i \in [\,l\,], \\
p_i &:= h(s_i; w_h), \quad i \in [\,l\,].
\end{aligned}
\end{equation}
$$

donde $w ;= (w_g, w_h, s_0)$ son los parámetros de la red, $s_0$ es el estado inicial. El objetivo de $g$ es actualizar
el estado $s_i$ en función de la entrada $x$ y el estado anterior $s_{i-1},$ mientras que el objetivo de $h$ es mapear
el estado $s_i$ a un vector $p_i$ dado el estado $s_i.$. Es importante destacar que los parámetros de $g$ y $h$ se
cruzan a los largo de los pasos de tiempo. Típicamente, $g$ y $h$ se parametrizan utilizando MLPs de una capa oculta.
Nótese que $g$ tiene múltiples entradas.

### Secuencia a vector

En esta configuración, se define una función codificadora $p = f^e(x_{1:l}; w)$ que mapea una secuencia de vectores
$x_{1:l} \in \mathbb{R}^{l\times d}$ y un vector de parámetros $w \in \mathbb{R}^p$ a un vector $p \in \mathbb{R}^m.$

<ImageBox
  src="/images/differentiable-programs/RNNv2.svg"
  alt="Computation Chain"
  width="500px"
  height="350px"
>
  **Figura 4**. Muchos a uno (codificación). Una secuencia de vectores $x_{1:l} \in \mathbb{R}^{l\times d}$ es mapeada a un vector $p \in \mathbb{R}^m.$
</ImageBox>

Esto resulta útil para la clasificación de secuencias, donde se clasifica la secuencia completa en una sola categoría,
por ejemplo en el análisis de sentimientos. Formalmente, podemos definir $p = f^e(x_{1:l}; w)$ a través de la recursión:

$$
\begin{equation}
\begin{aligned}
s_i &:= \gamma(x_i, s_{i-1}; w_g), \quad i \in [\,l\,], \\
p &:= \text{pooling}(s_{1:l}),
\end{aligned}
\end{equation}
$$

donde $w := (w_g, s_0)$ son los parámetros de la red, $s_0$ es el estado inicial y $\gamma$ es una función similar a
$g$, excepto que ahora actualiza los estados del codificador y no toma predicciones anteriores como entrada. La función
de agrupamiento ($\text{pooling}$) típicamente no tiene parámetros y se utiliza para reducir la secuencia de estados a
un solo vector. Ejemplos comunes de funciones de agrupamiento son la media, la suma y la función de max-pooling.

### Secuencia a secuencia (alineada)

En esta configuración, se define una función de alineación $p_{1:l} = f^a(x_{1:l}; w)$ que mapea una secuencia de
vectores $x_{1:l} \in \mathbb{R}^{l\times d}$ y un vector de parámetros $w \in \mathbb{R}^p$ a una secuencia de vectores
$p_{1:l} \in \mathbb{R}^{l\times m}.$ La longitud de la secuencia de entrada y la secuencia de salida son iguales.

<ImageBox
  src="/images/differentiable-programs/RNNv3.svg"
  alt="Computation Chain"
  width="500px"
  height="350px"
>
  **Figura 4**. Muchos a muchos (alineados). Una secuencia de vectores $x_{1:l} \in \mathbb{R}^{l\times d}$ es mapeada a una secuencia de vectores $p_{1:l} \in \mathbb{R}^{l\times m}.$
</ImageBox>

Un
ejemplo de aplicación es **POS-tagging**, donde se asigna una etiqueta a cada palabra en una oración. Formalmente, esto
se puede definir como $p_{1:l} = f^a(x_{1:l}; w)$ a través de la recursión:

$$
\begin{equation}
\begin{aligned}
s_i &:= g(x_i, s_{i-1}; w_g), \quad i \in [\,l\,], \\
p_i &:= h(s_i; w_h), \quad i \in [\,l\,].
\end{aligned}
\end{equation}
$$

donde $w := (w_g, w_h, s_0)$ son los parámetros de la red, $s_0$ es el estado inicial y $g$ y $h$ son funciones
similares a las definidas en la configuración uno a muchos. La función $g$ actualiza el estado $s_i$ en función de la
entrada $x_i$ y el estado anterior $s_{i-1},$ mientras que la función $h$ mapea el estado $s_i$ a un vector $p_i$ dado
el estado $s_i.$

### Secuencia a secuencia (no alineada)

En esta configuración, se define una función de alineación $p_{1:l'} = f^u(x_{1:l}; w)$ que mapea una secuencia de
vectores $x_{1:l} \in \mathbb{R}^{l\times d}$ y un vector de parámetros $w \in \mathbb{R}^p$ a una secuencia de vectores
$p_{1:l'} \in \mathbb{R}^{l'\times m}.$

<ImageBox
  src="/images/differentiable-programs/RNNv4.svg"
  alt="Computation Chain"
  width="500px"
  height="600px"
>
  **Figura 5**. Muchos a muchos (no alineados). Una secuencia de vectores $x_{1:l} \in \mathbb{R}^{l\times d}$ es mapeada a una secuencia de vectores $p_{1:l'} \in \mathbb{R}^{l'\times m}.$
</ImageBox>

La longitud de la secuencia de entrada y la secuencia de salida no
necesariamente son iguales. Un ejemplo de aplicación es la traducción automática, donde una oración en un idioma se
traduce a una oración en otro idioma. Formalmente, esto se puede definir como $p_{1:l'} = f^u(x_{1:l}; w)$ a través de
la recursión:

$$
\begin{equation}
\begin{aligned}
c &:= f^e(x_{1:l}; w_e), \\
p_{1:l} &:= f^d(c, w_d),
\end{aligned}
\end{equation}
$$

donde $w := (w_e, w_d)$ son los parámetros de la red, $f^e$ es una función codificadora que mapea la secuencia de
entrada a un vector de contexto $c,$ y $f^d$ es una función decodificadora que mapea el vector de contexto a la
secuencia de salida. La función de codificación $f^e$ y la función de decodificación $f^d$ pueden ser implementadas como
RNNs. Los pasos en detalle, serían:

$$
\begin{equation}
\begin{aligned}
s_i &:= g(x_i, s_{i-1}; w_g), \quad i \in [\,l\,], \\
c &:= \text{pooling}(s_{1:l}), \\
z_i &:= g(c, p_{i-1}, z_{i-1}; w_g), \quad i \in [\,l'\,], \\
p_i &:= h(z_i; w_h), \quad i \in [\,l'\,].
\end{aligned}
\end{equation}
$$

Esta arquitectura se denomina acertadamente arquitectura codificador-decodificador (_encoder-decoder_). Tenga en cuenta
que denotamos la longitud de la secuencia objetivo como $l'$ en lugar de $l$ para evitar confusiones. Sin embargo, en la
práctica, la longitud puede depender de la entrada y a menudo no se conoce de antemano. Para abordar este problema, el
vocabulario (de tamaño $D$ en nuestra notación) se aumenta típicamente con un token de «final de secuencia» (EOS, por
sus siglas en inglés), para que, en el momento de la inferencia, sepamos cuándo dajar de generar la secuencia de salida.
Una desventaja de esta arquitectura, es que toda la información sobre la secuencia de entrada esta contenida en el
vector de contexto $c,$ que por lo tanto puede convertirse en un cuello de botella.

## Conclusiones

Las redes neuronales, concebidas como programas parametrizados, proporcionan un marco poderoso y flexible para abordar una amplia gama de problemas en aprendizaje automático y procesamiento de datos. A lo largo de este artículo, hemos explorado diversas arquitecturas y configuraciones, desde redes de alimentación hacia adelante hasta redes neuronales recurrentes y redes residuales. La representación de estas arquitecturas como grafos dirigidos acíclicos (DAGs) no solo facilita su análisis y optimización, sino que también permite la aplicación eficiente de técnicas como la diferenciación automática. Las diferentes configuraciones discutidas demuestran la versatilidad de estos modelos para tareas que van desde la generación de subtítulos de imágenes hasta la traducción automática.

Este enfoque de ver las redes neuronales como programas parametrizados no solo permite diseñar modelos más eficientes y efectivos, sino que también abre caminos para mejorar la interpretabilidad y explicabilidad de estos modelos, un área de creciente importancia en la inteligencia artificial. Aunque hemos cubierto varios tipos de redes neuronales, es importante destacar que el campo está en constante evolución, con nuevas arquitecturas y técnicas desarrollándose continuamente. En resumen, esta perspectiva proporciona una base sólida para el desarrollo futuro de modelos de aprendizaje automático más avanzados, interpretables y adaptables a una amplia gama de aplicaciones en el mundo real.

## Imágenes

- [Unplash](https://unsplash.com/photos/blue-neurons-with-glowing-segments-over-blue-background-neuron-interface-and-computer-science-concept-3d-rendering-copy-space-G1FpRJcLoCw) - Getty Images - Neurons.

## Referencias

1. [Mathieu Blondel and Vincent Roulet. 2024. The Elements of Differentiable Programming.](https://arxiv.org/pdf/2403.14606)
