---
title: 'Redes neuronales como programas parametrizados'
date: '2024/06/23'
tags: ['matemáticas', 'ciencias de la computación']
draft: false.
time: '15 min'
summary: 'Las redes neuronales pueden ser pensadas como programas parametrizados: programas con parámetros por aprender. En este artículo, exploraremos cómo representar los programas parametrizados matemáticamente. En particular, veremos como los programas pueden ser representados como grafos de computación.'
bibliography: references-data.bib
canonicalUrl: 'https://www.asanchezyali.com/blog/es/number-theory/20230108DivisionAlgorithm/'
headerImage: '/images/differentiable-programs/header.avif'
images: ['/images/differentiable-programs/header.avif']
language: 'es'
---

Las redes neuronales pueden ser pensadas como programas parametrizados, es decir, programas con parámetros ajustables
que se aprenden. En este
artículo, exploraremos cómo representar matemáticamente estos programas. En particular, veremos como los programas pueden ser
representados como grafos dirigidos no cíclicos y cómo la diferenciación automática puede ser utilizada para calcular derivadas
de programas de manera eficiente.

## Representación de programas computacionales

### Cadenas de computación

Para comenzar, consideremos programas simples dados por una **secuencia** de funciones $f_1, \ldots, f_n$ que se aplican
de forma secuencial a una entrada
$s_0 \in S_{0}.$ A estos programas los llamaremos **cadenas de computación** y los denotaremos como $f = f_n \circ \cdots
\circ f_1.$ Por ejemplo, una imagen puede pasar a través de una secuencia de transformaciones tales como recortar,
rotar, normalizar, etc. En el caso de las redes neuronales, las transformaciones suelen estar parametrizadas y los
parámetros se aprenden a través de un proceso de optimización.

<ImageBox
  src="/images/differentiable-programs/figure1.svg"
  alt="Computation Chain"
  width="700px"
  height="150px"
>
  **Figura 1**. Una cadena de computación es una secuencia de composiciones de funciones. En el
  gráfico anterior, cada nodo intermedio representa una función. La primera flecha representa la
  entrada, y la última la salida. Las flechas interiores representan las dependencias de las
  funciones con respecto a las salidas previas o el entrada inicial.
</ImageBox>

Formalmente, una cadena de computación puede ser escrita como:

$$
\begin{equation}
\begin{aligned}
s_0 &\in S_{0}, \\
s_{1} &= f_{1}(s_{0}) \in S_{1}, \\
s_{2} &= f_{2}(s_{1}) \in S_{2}, \\
&\vdots \\
s_{n} &= f_{n}(s_{n-1}) = f(s_0) \in S_{n}.\\
\end{aligned}
\end{equation}
$$

Donde $s_0$ es la **entrada**, $s_k\in S_k$ para $k = 1, \ldots,
 n-1,$ son los **estados intermedios** del programa y $s_n\in S_n$ es la
**salida**. Por supuesto, el dominio (espacios de entrada) de $f_k$ debe ser compatible con la imagen (espacio de
salida) de $f_{k-1}$ para que la composición tenga sentido. Es decir, que $f_k: S_{k-1} \to S_k$ para $k = 1, \ldots,
 n.$ De manera equivalente a la ecuación (1), podemos escribir la cadena de computación como:

$$
\begin{equation}
s_n = f(s_0) = f_n \circ \cdots \circ f_1(s_0).
\end{equation}
$$

Una cadena de computación puede ser representada como un **grafo dirigido acíclico (DAG)**, donde los nodos representan
las funciones y las aristas representan las dependencias entre las funciones.

### Grafos acíclicos dirigidos

En un programa genérico, las funciones intermedias pueden depender de las salidas de otras funciones. Tales dependencias
pueden ser representadas como un grafo dirigido acíclico (DAG). Un **grafo dirigido** $G = (V, E)$ consiste de un
conjunto de nodos $V$ y un conjunto de aristas $E\subseteq V \times V.$ Un arista $(u, v) \in E$ es un par ordenad de
vertices $u, v\in V$. Es también representado como $u \to v.$, para indicar que $u$ depende de $v.$ Para representar las
entradas y las salidas, será conveniente usar **semi-aristas entrantes** $\to j$ y **semi-aristas salientes** $i \to.$

En un grafo $G = (V, E)$, los **padres** de un nodo $v\in V$ son los nodos $u\in V$ tales que $(u, v)\in E,$ denotado
por

$$
\begin{equation}
\text{padres}(v) = \{u\in V: (u, v)\in E\}.
\end{equation}
$$

De manera similar, los **hijos** de un nodo $v\in V$ son los nodos $w\in V$ tales que $(v, w)\in E,$ denotado por

$$
\begin{equation}
\text{hijos}(v) = \{w\in V: (v, w)\in E\}.
\end{equation}
$$

Los vertices sin padres son llamados **raíces** y los vertices sin hijos son llamados **hojas**.

Un **camino** de $i$ a $j$ en un grafo $G = (V, E)$ es una secuencia de nodos $v_1, \ldots, v_k$ tal que $i \to v_1 \to
\cdots \to v_k \to j.$ Un camino es **simple** si no contiene nodos repetidos. Un grafo es **acíclico** si no contiene
ciclos, es decir, no hay caminos simples de un nodo a sí mismo. Un grafo acíclico dirigido (DAG) es un grafo dirigido
que es acíclico.

La aristas de un DAG inducen un **orden parcial** en los nodos, que denotaremos por $i \preceq j$ si existe un camino de
$i$ a $j.$ Un orden parcial es una relación reflexiva, transitiva y antisimétrica. El orden es parcial porque no todos
los nodos están relacionados entre sí. No obstante, podemos definir un **orden total** llamado **orden topológico**:
cualquier orden tal que $i \preceq j$ si y solo si no hay un camino de $j$ a $i.$

### Programas computacionales como DAGs

Nosotros que un programa computacional es una función valida en el sentido matemático, es decir, el programa debería
retornar valores idénticos para las mismas entradas y no debería tener efectos secundarios. También asumimos que el
programa se detiene en un número finito de pasos. Por lo tanto, un programa puede estar hecho de un número finito de
funciones intermedias y variables intermedias, de tal forma que las dependencias entres las funciones y las variables
pueden ser representados como un grafo dirigido acíclico. Sin perdida de generalidad, podemos hacer las siguientes
suposiciones:

1. Existe un nodo **entrada** $s_0$ y un nodo **salida** $s_n.$
2. Cada función intermedia $f_k$ produce un solo valor $s_k \in S_k.$

Con un número finito de nodos como $V = \{0, 1, \ldots, n\}$. El node $0$ es una raíz, correspondiente a la
entrada del programa, y el nodo $n$ es una hoja, correspondiente a la salida del programa. Las funciones intermedias.
Debido al tercer supuesto, aparte de $s_0$, cada variable $s_k$ está en biyección con una función $f_k.$ Por lo tanto,
el node $0$ representa la entrada $s_0$, y los nodos $1, \ldots, n$ representan las funciones $f_1, \ldots,
f_n$ y salidas $s_1, \ldots, s_n,$ es decir un nodo $k$ representa la función $f_k$ y la salida $s_k.$

Las aristas de un DAG representan las dependencias. Los padres $\{i_1, \ldots, i_{p_k}\} := \text{padres}(k)$ de un nodo
$k,$ donde $p_k:=|\text{padres}(k)|,$ indican las variables $s_{\text{padres}(k)} := \{s_{i_1}, \ldots, s_{i_{p_k}}\}$
que la función $f_k$ necesita para computar su salida $s_k$. Dicho de otro modo, los padres $\{i_1, \ldots, i_{p_k}\}$
indican funciones $f_{i_1}, \ldots, f_{i_{p_k}}$ que son necesarias para computar $f_k.$

<PseudoCode title="Algoritmo 1. Ejecución de un programa">
  <PseudoCodeLine showNumber={false} indent={0}>
    $\text{\textbf{Funciones:}}$ $f_1, \ldots, f_n \text{ en orden topológico}$
  </PseudoCodeLine>
  <PseudoCodeLine showNumber={false} indent={0}>
    $\text{\textbf{Entrada:}}$ $s_0\in S_0$
  </PseudoCodeLine>
  <PseudoCodeLine number={1}>
    $\text{\textbf{Para} } k = 1, \ldots, n$ $\text{\textbf{hacer}}$
  </PseudoCodeLine>
  <PseudoCodeLine number={2} indent={2}>
    <span>Encontrar los nodos padres</span> $\{i_1, \ldots, i_{p_k}\} := \text{padres}(k)$
  </PseudoCodeLine>
  <PseudoCodeLine number={3} indent={2}>
    <span>Calcular</span> $s_k :=f_k\big(s_{padres(k)}\big):= f_k(s_{i_1}, \ldots, s_{i_{p_k}})$
  </PseudoCodeLine>
  <PseudoCodeLine howNumber={false} indent={0}>
    $\text{\textbf{Fin:} } f(s_0) = s_n$
  </PseudoCodeLine>
</PseudoCode>

#### Ejecución de un programa

Para ejecutar un programa, nosotros necesitamos asegurar que las funciones intermedias son evaluadas en un orden en el
orden correcto. Por lo tanto, nosotros asumimos que los nodos $V=\{0, 1, \ldots, n\}$ están ordenados en un **orden
topológico**, (si este no es el caso, entonces el programa no puede ser ejecutado). Nosotros podemos ejecutar un
programa para evaluar la salida $s_k \in [n]$

$$
\begin{equation}
s_k := f_k\big(s_{\text{padres}(k)}\big):=f_k(s_{i_1}, \ldots, s_{i_{p_k}})\in S_k.
\end{equation}
$$

Obsérvese que podemos ver $f_k$ como una función de una sola entrada de $s_{\text{padres}(k)},$ el cual es una tupla de
elementos, o como una función de múltiples entradas $s_{i_1}, \ldots, s_{i_{p_k}}.$ Los dos puntos de vistas son
esencialmente equivalentes. El proceso de ejecución de un programa es descrito en el Algoritmo 1.

#### Tratamiento de múltiples entradas o salidas del programa

Cuando un programa tiene múltiples entradas, los agrupamos siempre en una sola entrada $s_0 \in S_0.$ como $s_0 = (s_{0,
1}, \ldots, s_{0, m_0})$ con $S_0 = S_{0, 1} \times \cdots \times S_{0, m_0},$ ya que las funciones posteriores siempre
pueden filtrar los elementos de $s_0$ que necesitan. Del mismo modo, si una función intermedia $f_k$ tiene varias
salidas, siempre las agrupamos en una sola salida $s_k \in S_k$ como $s_k = (s_{k, 1}, \ldots, s_{k, m_k})$ con $S_k =
S_{k, 1} \times \cdots \times S_{k, m_k},$ ya que las funciones posteriores siempre pueden filtrar los elementos de
$s_k$.

<ImageBox
  src="/images/differentiable-programs/figure2.svg"
  alt="Computation Chain"
  width="700px"
  height="150px"
>
  **Figura 2**. Dos posibles representaciones de un programa. **Izquierda:** La funciones son
  representadas como nodos y las dependencias como aristas. **Derecha:** Las funciones son
  representadas como nodos y las dependencias como un conjunto de nodos disjuntos. En ambos casos,
  la entrada es representada por un nodo raíz y la salida por un nodo hoja.
</ImageBox>

#### Representación alternativa de programas: grafos bipartitos

En nuestro formalismo, dado que una función $f_k$ siempre tiene una única salida $s_k,$ se puede considerar que un nodo
$k$ representa tanto la función $f_k$ como la salida $s_k.$ Sin embargo, también es posible considerar dos conjuntos de
nodos disjuntos: uno para las funciones y otro para las salidas, es decir, un grafo bipartito. Este formalimso es
similar a los grafos factoriales (_Frey et al_.,
[1997](https://people.ee.ethz.ch/~loeliger/localpapers/FG_Allerton1997.pdf);
_Loeliger_,
[2004](https://ieeexplore.ieee.org/document/1267047))
utilizados en la modelización probabilísticas, pero con aristas dirigidas. Una ventaja de este formalismo es que permite
que las funciones tengan múltiples salidas. Por simplicidad, nos centraremos en la representación de un solo conjunto de nodos.

### Circuitos aritméticos

Los circuitos aritméticos son uno de los ejemplos más sencillos de computación grafo, originarios de **la teoría de la
complejidad computacional**. Formalmente, un circuito aritmético sobre un campo $\mathbb{F}$, como por ejemplo los
número reales $\mathbb{R}$ o los números complejos $\mathbb{C},$ es un grafo dirigido acíclico (DAG) donde los nodos
raíces son elementos del campo $\mathbb{F}$ y las funciones intermedias son operaciones aritmética como la suma o la
multiplicación. Estas últimas suelen denominarse **compuertas aritméticas**. Contrariamente al caso general de grafos
computacionales, cada función sea una suma o un producto puede tener múltiples nodos raíces. Los nodos raíces pueden ser
variables o constantes, y deben pertenecer al campo $\mathbb{F}.$ Los circuitos aritméticos puede utilizarse para
calcular polinomios. Puede existir múltiples circuitos aritméticos para representar un polinomio dado. Para comparar dos
circuitos aritméticos que representan el mismo polinomio, una noción intuitiva de la **complejidad** es el **tamaño del
circuito**, como se define a continuación.

<MathBox title="Definición 1. Circuito y tamaño polinomial" variant="definition">
  El **tamaño** $S(C)$ de un circuito aritmético $C$ es el número de aristas en el grafo dirigido
  acíclico (DAG) que representa al circuito. EL **tamaño del polinomio** $S(f)$ de un polinomio $f$
  es el tamaño del circuito aritmético $C$ más pequeño que representa a $f.$
</MathBox>

Para más información sobre circuitos aritméticos, se puede consultar el libro de _Chen et al_.
[2011](https://www.math.ias.edu/~avi/PUBLICATIONS/ChenKaWi2011.pdf).

## Redes de alimentación hacia adelante

Una red de alimentación hacia adelante (_feedforward network_) es un tipo de cadena de computación con funciones parametrizadas $f_1, \ldots, f_n$ que se aplican de forma secuencial a una
entrada $s_0 \in S_0.$ En este caso, las funciones $f_k$ son usualmente funciones parametrizadas que dependen de un
vector de parámetros $w_k \in \mathcal{W}_k,$ donde $\mathcal{W}_k$ es el espacio de parámetros de la función $f_k.$ Por lo
tanto,

$$
\begin{equation}
\begin{aligned}
x &= s_0 \in S_{0}, \\
s_{1} &= f_{1}(s_{0}; w_1) \in S_{1}, \\
s_{2} &= f_{2}(s_{1}; w_2) \in S_{2}, \\
&\vdots \\
s_{n} &= f_{n}(s_{n-1}; w_n) = f(s_0; w) \in S_{n}.\\
\end{aligned}
\end{equation}
$$

para una entrada $s_0\in S_0$ y para los **parámetros entrenables** $w = (w_1, \ldots, w_n) \in \mathcal{W} =
\mathcal{W}_1 \times \cdots \times \mathcal{W}_n.$ Cada función $f_k$ es una **capa** de la red y cada $s_k \in S_k$ es
una **representación intermedia** de la entrada $s_0.$ la dimension de $S_k$ se conoce como la **dimensión de la capa**
(o número de unidades ocultas) de la capa $k.$ Una red de alimentación hacia adelante puede ser definida como una
función $f:S_0 \times \mathcal{W} \to S_n$ que mapea una entrada $s_0$ y parámetros $w$ a una salida $s_n.$

Dado un programa parametrizado de este tipo, los parametros $w$ podemos aprender los parámetros ajustando $w$
de acuerdo a un conjunto de datos de entrenamiento. Por ejemplo, dado un conjunto de datos de pares $(x_i, y_i),$ podemos
minimizar la función de pérdida $L(w) = \sum_{i=1}^{m} \ell(f(x_i; w), y_i).$ La minimización de la función de
pérdida se puede hacer utilizando un algoritmo de optimización como el descenso de gradiente.

## Perceptrones multicapa

### Combinación de capas afines y funciones de activación

En la sección anterior, no especificamos cual es la forma de parametrizar las redes de alimentación hacia adelante. Una
parametrización típica, es el perceptron multicapa (_multilayer perceptron_ o MLP), que utiliza capas totalmente
conectadas (también conocidas densas) de la forma

$$
\begin{equation}
s_k = f_k(s_{k-1}; w_k) = a_k(W_k s_{k-1} + b_k),
\end{equation}
$$

donde $w_k = (W_k, b_k)$ son los parámetros de la capa $k,$ $W_k$ es un matriz de pesos y $b_k$ es un vector de sesgos.
Ademas la capa las podemos descompones en dos funciones. La función $s \mapsto W_k s + b_k$ es una **capa afín** y la
función $s \mapsto a_k(s)$ es no lineal sin parámetros, llamada **función de activación**.

Generalmente, podemos remplazar el producto matrix-vector $W_k s_{k-1}$ por cualquier función lineal de $s_{k-1}.$ Por
ejemplo, las capas convolucionales utilizan de convolución de la entrada $s_{k-1}$ con algunos filtros $W_k.$

<MathBox title="Remark 1. Tratamiento de entradas múltiples" variant="remark">
  A veces es necesario tratar con redes de múltiples entradas. POr ejemplo, supongamos que queremos
  diseñar una función $g(x_1, x_2, w_g)$, donde $x_1 \in \mathcal{X}_1$ y $x_2 \in \mathcal{X}_2$
  son dos entradas. Una forma simple de hacerlo es usar la concatenación $x = x_1 \oplus x_2 \in
  \mathcal{X}_1 \oplus \mathcal{X}_2$ como entrada a una red $f(x, W_f).$ Alternativamente, en lugar
  de concatenar las entradas, se pueden concatenar después de haber pasado por una o más capas
  ocultas.
</MathBox>

### Relación con los modelos lineales generalizados

Cuando la profundidad de la red es $n=1$ (es decir, una sola capa), la salida un MLP es una función lineal de la entrada
$x$ es

$$
\begin{equation}
s_1 = a_1(W_1 x + b_1).
\end{equation}
$$

Esto es llamado un **modelo lineal generalizado** (_generalized linear model_ o GLM). En este caso, la función de
activación $a_1$ es la función de identidad. Los modelos lineales generalizados son un caso especial de los MLPs. Es
particular, cuando $a_1$ es la función $\operatorname*{softargmax}$, se tiene un modelo de regresión logística. Por lo
general para la profundidad $n>1,$ la salida de un MLP es

$$
\begin{equation}
s_n = a_n(W_n s_{n-1} + b_n).
\end{equation}
$$

Esto puede ser visto como un GLM sobre la **representación intermedia** $s_{n-1}$ de la entrada $s_0.$ Este es el
principal atractivo de los MLPs: la capacidad de aprender representaciones intermedias que son útiles para la tarea de
interés. Veremos que los MLPs también pueden utilizarse como subcomponentes en otras arquitecturas.

## Funciones de activación

Como se mencionó anteriormente, la redes de alimentación hacia adelante utilizan funciones de activación $a_k$ para cada
capa. En esta sección, vamos a describir algunas de las funciones de activación más comunes de escalar a escalar o de
vector a escalar. También presentaremos algunas funciones de probabilidad que pueden ser utilizadas como funciones de
activación.

### Funciones de activación de escalares a escalares nolineales

La función **ReLU** (_Rectified Linear Unit_) es una función de activación no lineal que se define como la parte no
negativa de su argumento. Es decir, la función ReLU es la función identidad en los valores no negativos y cero en los
valores negativos:

$$
\begin{equation}
\text{ReLU}(x) = \max(0, x) = \begin{cases}
x & \text{si } x \geq 0, \\
0 & \text{si } x < 0.
\end{cases}
\end{equation}
$$

Es una función lineal por partes e incluye un punto de inflexión en $x=0.$ Una perceptron multicapa con activaciones
ReLU se denomina red neuronal rectificadora. Las capas toman las forma

$$
\begin{equation}
s_k = \text{ReLU}(W_k s_{k-1} + b_k).
\end{equation}
$$

donde la ReLU es aplicada elemento a elemento. La función ReLU puede ser remplazada con una versión suavizada, conocida
como **softplus**:

$$
\begin{equation}
\text{softplus}(x) = \log(1 + \exp(x)).
\end{equation}
$$

A diferencia de la ReLU, la softplus es una función suave, diferenciable y estrictamente positiva.

### Funciones de activación de vectores a escalares nolineales

A menudo es útil reducir vectores a escalares. Estos valores escalares pueden ser vistos como puntajes o probabilidades,
que representan la importancia de un vector de entrada. Una función común para hacer esto es usando el valor máximo,
también conocido como **max-pooling**. Dado un vector $x\in \mathbb{R}^d,$ la función de max-pooling es

$$
\begin{equation}
\text{maxpooling}(x) = \max_{j \in [\,d\,]} x_j.
\end{equation}
$$

Como una aproximación suave de la función de max-pooling, se puede usar la función **log-sum-exp**, la cual se comporta
como una versión suavizada de la función max-pooling:

$$
\begin{equation}
\text{logsumexp}(x) := \text{softmax}(x) := \log\sum_{j=1}^{d} \exp(x_j).
\end{equation}
$$

La función log-sum-exp puede ser vista como una generalización de la función **softplus** a vectores, en efecto para
todo $x \in \mathbb{R}$

$$
\begin{equation}
\operatorname*{logsumexp}((x, 0)) = \operatorname*{softplus}(x).
\end{equation}
$$

Una implementación númericamente estable de la función log-sum-exp es la siguiente:

$$
\begin{equation}
\text{logsumexp}(x) = \text{logsumexp}(x - c\mathbf{1}) + c,
\end{equation}
$$

donde $\mathbf{1}$ es un vector de unos y $c = \max_{j\in [\,d\,]} x_j.$

### Mapeos des probabilidad de escalar a escalar

En algunas ocasiones queremos mapear algún número real en un número entre 0 y 1, que puede representar la probabilidad
de un evento. Para se propósito se usa con frecuencia las **sigmoides**. Una sigmoide es una función cuya curva se
caracteriza por tener forma de «S». Estas funciones se utilizan para comprimir valores reales en un intervalo. Un
ejemplo es la función paso binaria, también conocida como la función de paso de Heaviside,

$$
\begin{equation}
\text{step}(x) = \begin{cases}
1 & \text{si } x \geq 0, \\
0 & \text{si } x < 0.
\end{cases}
\end{equation}
$$

Es un mapeo de $\mathbb{R}$ a $\{0, 1\}.$ Desafortunadamente, la función de paso binaria no es discontinua en $x=0.$
Además, debido a que la función es constante en todos los demás puntos, tiene derivada cero en estos puntos, lo que
dificulta su uso como parte de una red neuronal con retropropagación.

Una mejor sigmoide es la **función logítica**, la cual mapea de $\mathbb{R}$ a $(0, 1)$ y es definida como:

$$
\begin{equation}
\begin{aligned}
\text{logistic}(x) &= \frac{1}{1 + \exp(-x)} \\
&= \frac{\exp(x)}{1 + \exp(x)} \\
&= \frac{1}{2} + \frac{1}{2}\tanh\left(\frac{x}{2}\right).
\end{aligned}
\end{equation}
$$

Ella mapea $(-\infty, 0)$ a $(0, 0.5)$ y $\lbrack 0, \infty \rparen$ a $\lbrack 0.5, 1\rparen.$ y satisface que $\text{logistic}(0) = 0.5.$ Por lo
tanto puede ser vista como una función de probabilidad. La función logística puede ser vista como una versión suavizada
de la función de paso $\text{step}(x).$ Además la función logística se puede obtener como la derivada de la función
$\text{softplus}(x),$ es decir, para todo $x \in \mathbb{R}$

$$
\begin{equation}
\text{logistic}(x) = \frac{d}{dx}\text{softplus}(x).
\end{equation}
$$

Dos propiedades importantes de la función logística es que para todo $x \in \mathbb{R}$

$$
\begin{equation}
\text{logistic}(-x) = 1 - \text{logistic}(x),
\end{equation}
$$

y

$$
\begin{equation}
\begin{aligned}
\frac{d}{dx}\text{logistic}(x) &= \text{logistic}(x)(1 - \text{logistic}(x)) \\
&= \text{logistic}(x)\text{logistic}(-x).
\end{aligned}
\end{equation}
$$

### Mapeos de probabilidad de vectores a vectores

En algunas ocasiones queremos mapear un vector en un vector de probabilidades. Este es una mapero de $\mathbb{R}^d$ a un
simplex de probabilidad, definido por:

$$
\begin{equation}
\Delta^{d} = \left\{p\in \mathbb{R}^d: \forall j \in [\,d\,], p_j \geq 0 \text{ y } \sum_{j=1}^{d} p_j = 1\right\}.
\end{equation}
$$

Dos ejemplos de funciones de mapeo de probabilidad de vector a vector son la función **$\text{argmax}$ y la función
**$\text{softargmax}.$ La función $\text{argmax}$ mapea un vector a un vector de probabilidad que es cero en todas las
entradas excepto en la entrada con el valor máximo que lo hace uno. En efecto,

$$
\begin{equation}
\text{argmax}(x) = \phi(\arg\max_{j\in [\,d\,]} x_j),
\end{equation}
$$

donde $\phi(j)$ denota el _one-hot encoding_ de un entero $j\in [\,d\,],$ es decir, $\phi(j)_i = 1$ si $i = j$ y cero en
otro caso, esto es

$$
\begin{equation}
\phi(j) = (0, \ldots, 0, 1, 0, \ldots, 0) = e_j \in \{0, 1\}^d.
\end{equation}
$$

Este mapeo coloca toda la
masa de probabilidad en la entrada con el valor máximo, en caso de empate, se selecciona la primera entrada con el valor
máximo. Desafortunadamente, la función $\text{argmax}$ no es diferenciable, lo que dificulta su uso en la
retropropagación.

Una versión suavizada de la función $\text{argmax}$ es la función \*\*$\text{softargmax}$. La función $\text{softargmax}$
está definida como

$$
\begin{equation}
\text{softargmax}(x) = \frac{\exp(x)}{\sum_{j=1}^{d} \exp(x_j)}.
\end{equation}
$$

Este operador es comúnmente conocido en la literatura como la función _softmax_, pero esta denominación es errónea: este
operadora realmente define una versión suavizada de la función $\text{argmax}.$ La salida de $\text{softargmax}$
pertenece al interior relativo del simplex de probabilidad $\Delta^{d},$ lo que significa que nunca puede alcanzar los
bordes del simplex. Si denotamos $\pi = \text{softargmax}(x),$ entonces $\pi_j \in (0, 1),$ es decir $0 < \pi_j < 1$
para todo $j \in [\,d\,].$ La función $\text{softargmax}$ es el gradiente de la función $\text{log-sum-exp}$, es decir,
para todo $x \in \mathbb{R}^d$

$$
\begin{equation}
\text{softargmax}(x) = \nabla \text{log-sum-exp}(x).
\end{equation}
$$

La función $\text{softargmax}$ se puede ver como una generalización de la función $\text{logistic}$ a vectores, en
efecto

$$
\begin{equation}
\text{softargmax}(x) = \begin{bmatrix}
\text{logistic}(x_1) \\
\vdots \\
\text{logistic}(x_d)
\end{bmatrix}.
\end{equation}
$$

<MathBox title="Remark 2. Grados de libertad e inversa de la función softargmax" variant="remark">
  La función $\text{softargmax}$ satisface la propiedad que para todo $x \in \mathbb{R}^d$ y para todo $c \in
  \mathbb{R},$
  
  $$
  \begin{equation}
  \pi = \text{softargmax}(x + c\mathbf{1}) = \text{softargmax}(x).
  \end{equation}
  $$
  Esto quiere decir que la función $\text{softargmax}$ tiene $d-1$ grados de libertad y que no es invertible. Sin
  embargo, debido a la anterior propiedad, sin perdida de generalidad, nosotros podemos imponer que $\pmb{x}^\top
  \pmb{1} = \sum_{j=1}^{d} x_j = 0$ (si no es el caso, podemos simplemente hacer $\pmb{x} \leftarrow x_i - \bar{x}$, en
  donde $\bar{x} = \frac{1}{d}\sum_{j=1}^{d} x_j$ es la media de $x$). Con esta restricción y junto con el hecho de

$$
\begin{equation}
\log(\pi_i) = x_i - \log\sum_{j=1}^{d} \exp(x_j),
\end{equation}
$$

obtenemos

$$
\begin{equation}
\sum_{j=1}^{d} \log(\pi_j) = - d \log \sum_{j=1}^{d} \exp(x_j).
\end{equation}
$$

Por lo tanto, la función $\text{softargmax}$ es invertible en el sentido de que podemos recuperar $x$ a partir de

$$
\begin{equation}
x_i = [\text{softargmax}^{-1}(\pi)]_i = \log(\pi_i) - \frac{1}{d}\sum_{j=1}^{d} \log(\pi_j).
\end{equation}
$$

</MathBox>

## Redes neuronales residuales

Como se mencionó anteriormente, las redes de alimentación hacia adelante son un tipo de cadena de computación. En este
caso, las funciones intermedias son funciones parametrizadas que se aplican de forma secuencial a una entrada. En
efecto, consideremos una red de alimentación hacia adelante con $n+1$ capas $f_1, \ldots, f_{n+1}$. Seguramente, siempre
que $f_{n+1}$ sea la función identidad, el conjunto de funciones que esta red puede representar es el mismo que el de
una red de $n$ capas $f_1, \ldots, f_n.$ Por lo tanto, podemos considerar una red de $n+1$ capas como una red de $n$. En
otras palabras,la profundidad, en teoría, no debería perjudicar el poder expresivo de las redes de alimentación hacia
adelante. Desafortunadamente, la suposición de que cada $f_k$ es una función identidad no es realista. Esto significa
que la redes más profundas a veces pueden ser más difíciles de entrenar que las más superficiales, haciendo que la
precisión se sature o degrade en función de la profundidad.

La idea clave de las redes neuronales residuales (_residual neural networks_ o ResNets) (_He et al._,
[2016](https://arxiv.org/pdf/1512.03385)) es introducir **bloques residuales** entre las capas, de tal forma que la salida
de una capa se convierte en la entrada de una capa posterior. Formalmente, un bloque residual es una función de la forma

$$
\begin{equation}
s_{k} = f_k(s_{k-1}; w_k):= s_{k-1} + h_k(s_{k-1}; w_k).
\end{equation}
$$

La función $h_k$ es la **función residual**, dado que modela la diferencia entre la entrada y la salida, es decir,
$h_k(s_{k-1}; w_k) = s_{k} - s_{k-1}.$ La función $h_k$ puede ser vista como una **corrección** que se suma a la entrada
$s_{k-1}$ para obtener la salida $s_k.$ La suma con $s_{k-1}$ se conoce como **conexión de salto** (_skip connection_ o
_shortcut_). Siempre que se posible ajusta $w_k$ para que $h_k(s_{k-1}; w_k) = 0,$ entonces la función $f_k$ se
convierte en la función identidad. Por ejemplo, si nosotros usamos:

$$
\begin{equation}
h_k(s_{k-1}; w_k) = C_ka_k(W_ks_{k-1} + b_k) + d_k,
\end{equation}
$$

donde $w_k = (W_k, b_k, C_k, d_k)$ son los parámetros de la capa $k,$ entonces basta con ajustar $C_k = 0$ y $d_k = 0$
para que $h_k(s_{k-1}; w_k) = 0.$ Los bloque residuales son conocidos para remediar el problema de gradiente de fuga.

En muchos artículos y paquetes de software se incluye una activación adicional y en su lugar se define el bloque
residual como:

$$
\begin{equation}
s_{k} = f_k(s_{k-1}; w_k):= a_k(s_{k-1} + h_k(s_{k-1}; w_k)),
\end{equation}
$$

donde $a_k$ es usualmente la función ReLU. Si se debe incluir esta activación adicional o no, es esencialmente una
decisión de modelado. En la práctica, los bloques residuales también pueden incluir operaciones adicionales como
normalización por lotes (batch norm) y capa convolucionales.

## Redes neuronales recurrentes

La redes neuronales recurrentes (_recurrent neural networks_ o RNNs) (_Rumelhart et al._,
[1986](https://www.nature.com/articles/323533a0)) son un tipo de red neuronal que se utiliza para modelar secuencias de

## Imágenes

- [Unplash](https://unsplash.com/photos/blue-neurons-with-glowing-segments-over-blue-background-neuron-interface-and-computer-science-concept-3d-rendering-copy-space-G1FpRJcLoCw) - Getty Images - Neurons.

## Referencias

1. [Mathieu Blondel and Vincent Roulet. 2024. The Elements of Differentiable Programming.](https://arxiv.org/pdf/2403.14606)
